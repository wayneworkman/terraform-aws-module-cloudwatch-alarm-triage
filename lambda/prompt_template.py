import json
from datetime import datetime, timezone

class PromptTemplate:
    @staticmethod
    def generate_investigation_prompt(alarm_event, investigation_depth):
        depth_instructions = {
            "basic": "Perform a quick investigation focusing on the immediate cause and quick remediation steps.",
            "detailed": "Perform a thorough investigation including root cause analysis and prevention recommendations.",
            "comprehensive": "Perform an exhaustive investigation including all aspects, historical patterns, cascading impacts, and long-term improvements."
        }
        
        depth_instruction = depth_instructions.get(investigation_depth, depth_instructions["comprehensive"])
        current_time = datetime.now(timezone.utc).isoformat()
        alarm_region = alarm_event.get('region', 'us-east-1')
        
        prompt = f"""You are an expert AWS solutions architect and site reliability engineer. A CloudWatch Alarm has triggered and requires immediate investigation.

## Current Time
{current_time}

## üåç CRITICAL REGION INFORMATION
**THE ALARM OCCURRED IN REGION: {alarm_region}**

You MUST start your investigation in the {alarm_region} region. All initial AWS CLI commands and boto3 calls should explicitly specify --region {alarm_region} or region_name='{alarm_region}'. While this may be a multi-region issue, begin by investigating resources in {alarm_region} where the alarm originated.

## Your Investigation Environment

You have access to a tool Lambda function that can execute commands in a Python 3.13 environment with:
1. **AWS CLI v2** - Full AWS CLI available for making API calls
2. **Python with boto3** - For complex AWS SDK operations
3. **Standard Linux utilities** - For text processing and analysis

The tool Lambda has comprehensive read-only access to AWS services EXCEPT:
- No S3 object content access (can list buckets/objects but not read content)
- No DynamoDB data reads (can describe tables but not read items)
- No Secrets Manager access
- No Parameter Store SecureString access

## How to Use the Tool

Call the aws_investigator tool with:
- type: "cli" for AWS CLI commands
- type: "python" for Python/boto3 scripts

Example tool calls (note the --region parameter):
1. CLI: {{"type": "cli", "command": "aws cloudwatch get-metric-statistics --region {alarm_region} --namespace AWS/Lambda --metric-name Errors --start-time 2024-01-01T00:00:00Z --end-time 2024-01-01T01:00:00Z --period 300 --statistics Sum --dimensions Name=FunctionName,Value=my-function"}}
2. Python: {{"type": "python", "command": "import boto3\\nimport json\\nclient = boto3.client('iam', region_name='{alarm_region}')\\nresult = json.dumps(client.get_role(RoleName='my-role')['Role'], default=str)"}}

For Python commands, always assign the output to the 'result' variable for it to be returned.

## CloudWatch Alarm Event to Investigate
```json
{json.dumps(alarm_event, indent=2, default=str)}
```

## Investigation Requirements

{depth_instruction}

Perform a systematic investigation by:

1. **Initial Assessment**
   - Identify the alarming resource and metric
   - Understand the threshold and breach conditions
   - Determine the severity and urgency

2. **Data Gathering** (Use the tool extensively here)
   - REMEMBER: Start all investigations in the {alarm_region} region using --region {alarm_region}
   - Get recent CloudWatch Logs for the affected resource (last 30 minutes) in {alarm_region}
   - Retrieve CloudWatch metrics for trend analysis (last 2 hours) in {alarm_region}
   - Check IAM roles and policies if permissions might be involved
   - Look up recent CloudTrail events for configuration changes (last 24 hours) in {alarm_region}
   - Examine resource configurations and current state in {alarm_region}
   - Check AWS Health Dashboard for service issues in {alarm_region}
   - Get resource tags for context in {alarm_region}

3. **Root Cause Analysis**
   - What specific condition triggered this alarm?
   - What is the underlying root cause?
   - Is this part of a larger issue or isolated incident?
   - Are there any cascading failures or related alarms?

4. **Impact Assessment**
   - Which services, applications, or users are affected?
   - What is the business/operational impact?
   - What is the severity level (Critical/High/Medium/Low)?
   - Is there data loss or security risk?

5. **Historical Context**
   - Has this alarm triggered recently? (Check metrics history)
   - Are there patterns in the occurrences?
   - Were there recent deployments or changes?

6. **Remediation Steps**
   - Provide immediate actions to resolve the issue
   - Include specific AWS CLI commands or console steps
   - List preventive measures to avoid recurrence
   - Suggest monitoring improvements

## Output Format

Structure your response EXACTLY as follows:

### üö® EXECUTIVE SUMMARY
[2-3 sentences explaining the issue, its impact, and required action]

### üîç INVESTIGATION DETAILS

#### Commands Executed:
[List each tool command you executed and what you found]

#### Key Findings:
[Bullet points of important discoveries from your investigation]

### üìä ROOT CAUSE ANALYSIS
[Detailed explanation of why this alarm triggered based on your investigation]

### üí• IMPACT ASSESSMENT
- **Affected Resources**: [List specific resources]
- **Business Impact**: [Describe operational/business impact]
- **Severity Level**: [Critical/High/Medium/Low]
- **Users Affected**: [Estimate of affected users/systems]

### üîß IMMEDIATE ACTIONS
1. [Step-by-step remediation with specific commands]
2. [Include AWS CLI commands or console navigation]
3. [Time estimate for each action]

### üõ°Ô∏è PREVENTION MEASURES
- [Long-term fixes to prevent recurrence]
- [Architecture improvements if applicable]
- [Process improvements needed]

### üìà MONITORING RECOMMENDATIONS
- [Suggested alarm threshold adjustments]
- [Additional metrics to monitor]
- [Dashboard improvements]

### üìù ADDITIONAL NOTES
[Any other relevant information, caveats, or follow-up items]

## Important Instructions

1. **Use the tool extensively** - Don't make assumptions. Gather real data through AWS API calls.
2. **Be specific** - Provide exact commands, resource names, and values from your investigation.
3. **Be actionable** - Every recommendation should be immediately actionable with clear steps.
4. **Consider the context** - Adapt your investigation based on the specific alarm type and affected service.
5. **Check multiple sources** - Cross-reference findings from logs, metrics, and configuration.
6. **Time-box commands** - Use appropriate time ranges (last 30 mins for logs, 2 hours for metrics).
7. **Handle errors gracefully** - If a tool command fails, try alternative approaches.

Remember: This is a production incident. Be thorough but efficient. Focus on gathering facts through the tool, not speculation."""
        
        return prompt